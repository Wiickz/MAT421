{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wiickz/MAT421/blob/main/HW6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_68Ety6_Oz2"
      },
      "source": [
        "## Section 3.2: Continuity and Differentiation\n",
        "\n",
        "The optimization algorithm discussed in this module (steepest descent) makes heavy use of the gradient, a form of differentiation applied to vectors. In order for steepest descent to be used, the function must be differentiable, and in order for a function to be differentiable, it must be continuous (meaning there are no sudden changes in value or slope along the function):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjwOocFt_Oz3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "continuous = lambda x: x**2\n",
        "discontinuous = lambda x: np.abs(2*x - 5)\n",
        "\n",
        "x = np.linspace(-6, 10, 1000)\n",
        "\n",
        "plt.plot(x, continuous(x), label='Continuous')\n",
        "plt.plot(x, discontinuous(x), label='Discontinuous')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxWXIYCU_Oz4"
      },
      "source": [
        "A better definition of a continuous function is defined in the notes, where each point in the function has an existing limit that approaches the same value when taken from both sides.\n",
        "\n",
        "### Derivatives\n",
        "\n",
        "There is a reason that continuity is defined in this way: the derivative of a function is defined using the limit. By taking the slope between a point and an infinitely shrinking offset for every point in the function, a derivative function can be generated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wihkq3Jz_Oz4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "base = lambda x: x**2 - 4*x + 4\n",
        "\n",
        "def derivative(f, x, h):\n",
        "    return (f(x+h) - f(x)) / h\n",
        "\n",
        "x = np.linspace(-6, 10, 1000)\n",
        "h = 100\n",
        "\n",
        "plt.plot(x, base(x), label='Base')\n",
        "\n",
        "for i in range(1, 4):\n",
        "    plt.plot(x, derivative(base, x, h/(10**i)), label=f'Derivative Approximation {i}')\n",
        "\n",
        "true_df = lambda x: 2*x - 4\n",
        "plt.plot(x, true_df(x), label='True Derivative') # approximation 3 (h = 0.1) is already very close, thus the true derivative overlaps it - comment this out to see the difference\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmS6hpZp_Oz5"
      },
      "source": [
        "The derivative can also be used in multivariable functions like those often found in linear algebra settings. When attempting to observe the slope of a three-dimensional functions, the gradient is most helpful, as it creates a map of two-dimensional vectors that can be overlaid atop the function (from top-down) to show the direction of greatest increase in slope at various points in the function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViOfVVCb_Oz5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits import mplot3d\n",
        "\n",
        "x = np.outer(np.linspace(-3, 3, 30), np.ones(30))\n",
        "y = x.copy().T\n",
        "z = -x**3 + y**2\n",
        "\n",
        "fig = plt.figure()\n",
        "\n",
        "ax = plt.axes(projection='3d')\n",
        "ax.plot_surface(x, y, z, cmap='viridis')\n",
        "plt.show()\n",
        "\n",
        "gradient = lambda x, y: (-3*x**2, 2*y)\n",
        "\n",
        "x = np.linspace(-3, 3, 30)\n",
        "y = np.linspace(-3, 3, 30)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "U, V = gradient(X, Y)\n",
        "\n",
        "plt.quiver(X, Y, U, V)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKxnkAdA_Oz5"
      },
      "source": [
        "## Section 3.2.3: Taylor's Theorem\n",
        "\n",
        "The Taylor series of a function is an approximation of that function using a polynomial of variable degree. This representation is accomplished by taking increasing derivatives of a function divided by a corresponding increasing factorial value and summing the derivatives together:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhBwrPq3_Oz7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "f = lambda x: np.sin(x)\n",
        "h = 0.01 # good enough for a decent approximation of the derivative\n",
        "center = 0 # center (a) of Taylor approximation\n",
        "\n",
        "def derivative(f, x, h, n):\n",
        "    if n == 0:\n",
        "        return f(x)\n",
        "    else:\n",
        "        return (derivative(f, x+h, h, n-1) - derivative(f, x, h, n-1)) / h\n",
        "\n",
        "def taylor(f, x, a, n):\n",
        "    return sum(derivative(f, a, h, i) * (x-a)**i / math.factorial(i) for i in range(n+1))\n",
        "\n",
        "x = np.linspace(-2*np.pi + center, 2*np.pi + center, 1000)\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.ylim(-3, 3)\n",
        "plt.xlim(-2*np.pi + center, 2*np.pi + center)\n",
        "\n",
        "plt.plot(x, f(x), label='sin(x)')\n",
        "for i in range(1, 9, 2):\n",
        "    plt.plot(x, taylor(f, x, center, i), label=f'Taylor approximation (degree {i})')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEprrw54_Oz7"
      },
      "source": [
        "As seen above, as the degree of the Taylor approximation increases, the range in which the polynomial represents the true function accurately increases. This particular Taylor series is centered at a=0 and is also known as the Maclaurin series; Taylor series approximations at other points can be shown by changing `center` in the code above to a different value.\n",
        "\n",
        "## Section 3.3.3: Optimization by Gradient Descent\n",
        "\n",
        "There are various ways to optimize a continuous function to find a potential minimization; one such method is gradient descent, where a local minimum of a function is determined by following its gradient until a still point (the local minimum) is reached. This is also known as steepest descent, and a method to compute local minima can be implemented using Scipy and Numpy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZQI_9DK_Oz9"
      },
      "outputs": [],
      "source": [
        "### Excerpt from own work in MAT 423: Numerical Analysis I\n",
        "### Method of steepest descent to find a local minimum of a function\n",
        "\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "\n",
        "def steepest_descent_2(f, grad, x0, N=100, tol=1e-5):\n",
        "    '''Steepest descent method for a function of two variables'''\n",
        "\n",
        "    x = x0\n",
        "\n",
        "    for k in range(N):\n",
        "        u = grad(x[0], x[1])\n",
        "        if np.linalg.norm(u) < tol:\n",
        "            break\n",
        "        g = lambda t: f(x[0] - t*u[0], x[1] - t*u[1])\n",
        "        t = sp.optimize.minimize_scalar(g, bounds=(0, 1), method='bounded').x\n",
        "        x = x - t*u\n",
        "\n",
        "    return x, k\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    f = lambda x1, x2: np.sin(x1) + np.cos(x2)\n",
        "    grad = lambda x1, x2: np.array([np.cos(x1), -np.sin(x2)])\n",
        "\n",
        "    x0 = np.array([-1, 1]) # guess\n",
        "\n",
        "    x, n = steepest_descent_2(f, grad, x0)\n",
        "\n",
        "    print('Steepest Descent (k = %d)' % n)\n",
        "    print('x1         x2')\n",
        "    print('%.8f %.8f' % (x[0], x[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtcOXj9D_Oz9"
      },
      "source": [
        "As a bonus, the code here can also be adjusted to find a local maximum: by negating the output of `f` within the declaration of `g` and adding `t*u` instead of subtracting (`g = lambda t: -f(x[0] + t*u[0], x[1] + t*u[1])`, `x = x + t*u`), the algorithm will instead perform steepest ascent optimization."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}